{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$p(c_i|x,y)=\\frac{p(x,y|c_i)p(c_i)}{p(x,y)}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文本中构建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1] \n",
    "    return postingList, classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set() \n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document)\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: \n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOPosts, listClasses = loadDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cute',\n",
       " 'love',\n",
       " 'has',\n",
       " 'flea',\n",
       " 'problems',\n",
       " 'stupid',\n",
       " 'is',\n",
       " 'my',\n",
       " 'licks',\n",
       " 'ate',\n",
       " 'buying',\n",
       " 'mr',\n",
       " 'to',\n",
       " 'dalmation',\n",
       " 'so',\n",
       " 'maybe',\n",
       " 'park',\n",
       " 'please',\n",
       " 'dog',\n",
       " 'posting',\n",
       " 'him',\n",
       " 'food',\n",
       " 'steak',\n",
       " 'help',\n",
       " 'not',\n",
       " 'take',\n",
       " 'stop',\n",
       " 'I',\n",
       " 'quit',\n",
       " 'how',\n",
       " 'garbage',\n",
       " 'worthless']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVocabList = createVocabList(listOPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList, listOPosts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从词向量计算概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>\n",
    "$p(c_i|w)=\\frac{p(w|c_i)p(c_i)}{p(w)}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯分类器训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)  \n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0                    \n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += np.sum(trainMatrix[i])\n",
    "    p1Vect = np.log(p1Num / p1Denom)     \n",
    "    p0Vect = np.log(p0Num / p0Denom)    \n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMat = []\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0V, p1V, pAb = trainNB0(trainMat, listClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.56494936, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -2.56494936, -1.87180218, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -2.56494936, -2.56494936, -2.56494936, -2.56494936,\n",
       "       -3.25809654, -3.25809654, -2.56494936, -2.56494936, -3.25809654,\n",
       "       -2.15948425, -3.25809654, -2.56494936, -2.56494936, -3.25809654,\n",
       "       -3.25809654, -2.56494936, -2.56494936, -3.25809654, -2.56494936,\n",
       "       -3.25809654, -3.25809654])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.04452244, -3.04452244, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -1.65822808, -3.04452244, -3.04452244, -3.04452244, -3.04452244,\n",
       "       -2.35137526, -3.04452244, -2.35137526, -3.04452244, -3.04452244,\n",
       "       -2.35137526, -2.35137526, -3.04452244, -1.94591015, -2.35137526,\n",
       "       -2.35137526, -2.35137526, -3.04452244, -3.04452244, -2.35137526,\n",
       "       -2.35137526, -2.35137526, -3.04452244, -2.35137526, -3.04452244,\n",
       "       -2.35137526, -1.94591015])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = np.sum(vec2Classify * p1Vec) + np.log(pClass1) \n",
    "    p0 = np.sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V, p1V, pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(str(testEntry) + 'classified as: ' + str(classifyNB(thisDoc, p0V, p1V, pAb)))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(str(testEntry) + 'classified as: ' + str(classifyNB(thisDoc, p0V, p1V, pAb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation']classified as: 0\n",
      "['stupid', 'garbage']classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文档词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M.L.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent = 'This book is the best book on Python or M.L. I have ever laid eyes upon'\n",
    "mySent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/keras/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: split() requires a non-empty pattern match.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'Python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'I',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regEx = re.compile('\\W*')\n",
    "listOfTokens = regEx.split(mySent)\n",
    "listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in listOfTokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用朴素贝叶斯进行交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList = []\n",
    "    classList = [] \n",
    "    fullText = []\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, 'rb+').read().decode('utf-8', 'ignore'))\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, 'rb+').read().decode('utf-8', 'ignore'))\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []           \n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        \n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error\" + str(docList[docIndex]))\n",
    "    print('the error rate is: ' + str(float(errorCount) / len(testSet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error['experience', 'with', 'biggerpenis', 'today', 'grow', 'inches', 'more', 'the', 'safest', 'most', 'effective', 'methods', 'of_penisen1argement', 'save', 'your', 'time', 'and', 'money', 'bettererections', 'with', 'effective', 'ma1eenhancement', 'products', 'ma1eenhancement', 'supplement', 'trusted', 'millions', 'buy', 'today']\n",
      "the error rate is: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/keras/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类器 RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "nasa = feedparser.parse('feed://www.nasa.gov/rss/dyn/image_of_the_day.rss')\n",
    "len(nasa['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = feedparser.parse('http://www.independent.co.uk/news/uk/rss')\n",
    "len(news['entries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMostFreq(vocabList, fullText):\n",
    "    import operator\n",
    "    freqDict = {}\n",
    "    for token in vocabList:\n",
    "        freqDict[token] = fullText.count(token)\n",
    "    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True) \n",
    "    return sortedFreq[:30] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localWords(feed1,feed0):\n",
    "    import feedparser\n",
    "    docList=[]\n",
    "    classList = []\n",
    "    fullText =[]\n",
    "    minLen = min(len(feed1['entries']),len(feed0['entries']))\n",
    "    for i in range(minLen):\n",
    "        wordList = textParse(feed1['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(feed0['entries'][i]['summary'])\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)\n",
    "    top30Words = calcMostFreq(vocabList, fullText)   \n",
    "    for pairW in top30Words:\n",
    "        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n",
    "    trainingSet = list(range(2 * minLen))\n",
    "    testSet=[]          \n",
    "    for i in range(20):\n",
    "        randIndex = int(np.random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        \n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "    print('the error rate is: ' + str(float(errorCount) / len(testSet)))\n",
    "    return vocabList, p0V, p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is: 0.5\n"
     ]
    }
   ],
   "source": [
    "vocabList, p_nasa, p_news =  localWords(nasa, news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is: 0.55\n"
     ]
    }
   ],
   "source": [
    "vocabList, p_nasa, p_news =  localWords(nasa, news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is: 0.45\n"
     ]
    }
   ],
   "source": [
    "vocabList, p_nasa, p_news =  localWords(nasa, news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopWords(p0V,p1V):\n",
    "    import operator\n",
    "    vocabList, p0V, p1V = localWords(p0V, p1V)\n",
    "    top_p0V=[]\n",
    "    top_p1V=[]\n",
    "    for i in range(len(p0V)):\n",
    "        if p0V[i] > -6.0 : \n",
    "            top_p1V.append((vocabList[i], p0V[i]))\n",
    "        if p1V[i] > -6.0 : \n",
    "            top_p0V.append((vocabList[i], p1V[i]))\n",
    "    sortedp1V = sorted(top_p1V, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**\")\n",
    "    for item in sortedp1V:\n",
    "        print(item[0])\n",
    "    sortedp0V = sorted(top_p0V, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**\")\n",
    "    for item in sortedp0V:\n",
    "        print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is: 0.4\n",
      "p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**p1V**\n",
      "office\n",
      "second\n",
      "need\n",
      "police\n",
      "only\n",
      "not\n",
      "now\n",
      "life\n",
      "some\n",
      "becoming\n",
      "modern\n",
      "amber\n",
      "slip\n",
      "firefighters\n",
      "shadow\n",
      "condemnation\n",
      "danger\n",
      "tragic\n",
      "west\n",
      "decades\n",
      "yet\n",
      "rules\n",
      "europe\n",
      "street\n",
      "leader\n",
      "unacceptable\n",
      "two\n",
      "have\n",
      "stagnating\n",
      "into\n",
      "accident\n",
      "centres\n",
      "investigation\n",
      "growing\n",
      "information\n",
      "letter\n",
      "announces\n",
      "insists\n",
      "rates\n",
      "family\n",
      "expectancy\n",
      "trace\n",
      "wake\n",
      "video\n",
      "ever\n",
      "countries\n",
      "arlene\n",
      "momentum\n",
      "labour\n",
      "wilderness\n",
      "parts\n",
      "blame\n",
      "memories\n",
      "2015\n",
      "productive\n",
      "issue\n",
      "employees\n",
      "previously\n",
      "cartwright\n",
      "latest\n",
      "many\n",
      "before\n",
      "downing\n",
      "animated\n",
      "forecast\n",
      "took\n",
      "greenpeace\n",
      "joe\n",
      "hours\n",
      "said\n",
      "weather\n",
      "closure\n",
      "british\n",
      "public\n",
      "stronger\n",
      "north\n",
      "wales\n",
      "fertility\n",
      "warning\n",
      "ninety\n",
      "fierce\n",
      "them\n",
      "judge\n",
      "living\n",
      "home\n",
      "setting\n",
      "campsfield\n",
      "older\n",
      "divided\n",
      "remembrance\n",
      "repurposed\n",
      "spent\n",
      "thought\n",
      "history\n",
      "metropolitan\n",
      "don\n",
      "look\n",
      "johnson\n",
      "twenty\n",
      "panic\n",
      "all\n",
      "entered\n",
      "trend\n",
      "raf\n",
      "mean\n",
      "longest\n",
      "bells\n",
      "foster\n",
      "work\n",
      "referendum\n",
      "new\n",
      "because\n",
      "war\n",
      "interpol\n",
      "increases\n",
      "keep\n",
      "falling\n",
      "navigator\n",
      "south\n",
      "met\n",
      "following\n",
      "via\n",
      "commented\n",
      "than\n",
      "fall\n",
      "will\n",
      "attacker\n",
      "raises\n",
      "watch\n",
      "sweden\n",
      "say\n",
      "less\n",
      "islington\n",
      "bed\n",
      "house\n",
      "decorated\n",
      "migration\n",
      "seconds\n",
      "today\n",
      "conditions\n",
      "female\n",
      "chancellor\n",
      "short\n",
      "britain\n",
      "alarm\n",
      "independent\n",
      "concludes\n",
      "hemisphere\n",
      "lies\n",
      "their\n",
      "operations\n",
      "alexander\n",
      "paranormal\n",
      "hidden\n",
      "five\n",
      "18th\n",
      "been\n",
      "eight\n",
      "condemns\n",
      "verify\n",
      "child\n",
      "sept\n",
      "galileo\n",
      "conception\n",
      "1995\n",
      "money\n",
      "composite\n",
      "pad\n",
      "contributions\n",
      "outer\n",
      "arrived\n",
      "brilliant\n",
      "tuesday\n",
      "neutral\n",
      "ovchinin\n",
      "1898\n",
      "japanese\n",
      "nine\n",
      "oleg\n",
      "give\n",
      "artemyev\n",
      "telescope\n",
      "river\n",
      "plane\n",
      "cat\n",
      "three\n",
      "valleys\n",
      "six\n",
      "trust\n",
      "lure\n",
      "tests\n",
      "other\n",
      "ocean\n",
      "central\n",
      "delivered\n",
      "buoyancy\n",
      "series\n",
      "roscosmos\n",
      "orbit\n",
      "flat\n",
      "national\n",
      "flowing\n",
      "transport\n",
      "scorpius\n",
      "veggie\n",
      "deployed\n",
      "place\n",
      "known\n",
      "astronauts\n",
      "may\n",
      "deep\n",
      "force\n",
      "missions\n",
      "speaking\n",
      "members\n",
      "imager\n",
      "sky\n",
      "magellanic\n",
      "investigators\n",
      "jupiter\n",
      "cosmonaut\n",
      "prime\n",
      "minutes\n",
      "arnold\n",
      "unequal\n",
      "complain\n",
      "december\n",
      "core\n",
      "facility\n",
      "exploration\n",
      "returned\n",
      "stars\n",
      "ochoa\n",
      "artist\n",
      "botany\n",
      "curling\n",
      "which\n",
      "dark\n",
      "sun\n",
      "toward\n",
      "year\n",
      "sunday\n",
      "further\n",
      "day\n",
      "trip\n",
      "window\n",
      "magnified\n",
      "targeted\n",
      "onboard\n",
      "night\n",
      "more\n",
      "alexey\n",
      "even\n",
      "fifth\n",
      "180\n",
      "celebrating\n",
      "laden\n",
      "impression\n",
      "globular\n",
      "science\n",
      "its\n",
      "young\n",
      "service\n",
      "ahead\n",
      "growth\n",
      "southern\n",
      "autumn\n",
      "women\n",
      "con\n",
      "launched\n",
      "such\n",
      "birthday\n",
      "sunset\n",
      "inside\n",
      "gerst\n",
      "test\n",
      "ghost\n",
      "calling\n",
      "transfer\n",
      "train\n",
      "gas\n",
      "visits\n",
      "embrace\n",
      "another\n",
      "checklist\n",
      "feustel\n",
      "planets\n",
      "laboratory\n",
      "couple\n",
      "constellation\n",
      "visible\n",
      "sts\n",
      "polled\n",
      "atlantis\n",
      "assist\n",
      "floating\n",
      "final\n",
      "old\n",
      "rain\n",
      "heritage\n",
      "installs\n",
      "stop\n",
      "drew\n",
      "michael\n",
      "ethereal\n",
      "material\n",
      "beyond\n",
      "2017\n",
      "article\n",
      "senator\n",
      "billions\n",
      "star\n",
      "left\n",
      "feline\n",
      "john\n",
      "milky\n",
      "recovered\n",
      "how\n",
      "brother\n",
      "color\n",
      "survival\n",
      "paw\n",
      "ball\n",
      "venus\n",
      "one\n",
      "airbus\n",
      "might\n",
      "shipment\n",
      "party\n",
      "brexit\n",
      "bright\n",
      "nueces\n",
      "deal\n",
      "expected\n",
      "working\n",
      "organic\n",
      "storms\n",
      "cassieopeia\n",
      "center\n",
      "create\n",
      "ago\n",
      "vehicle\n",
      "nearly\n",
      "fire\n",
      "1998\n",
      "miles\n",
      "gulf\n",
      "landing\n",
      "solar\n",
      "called\n",
      "perform\n",
      "world\n",
      "captured\n",
      "campaigners\n",
      "amounts\n",
      "revolution\n",
      "1962\n",
      "cloud\n",
      "airport\n",
      "xavier\n",
      "better\n",
      "reading\n",
      "frozen\n",
      "planet\n",
      "raise\n",
      "fighting\n",
      "snow\n",
      "land\n",
      "photographed\n",
      "galaxy\n",
      "donut\n",
      "landsat\n",
      "finds\n",
      "mile\n",
      "boris\n",
      "theresa\n",
      "agency\n",
      "october\n",
      "shared\n",
      "ellen\n",
      "transits\n",
      "apps\n",
      "splashed\n",
      "boss\n",
      "gemini\n",
      "command\n",
      "glenn\n",
      "9th\n",
      "nebula\n",
      "murtha\n",
      "glow\n",
      "reflect\n",
      "team\n",
      "drifts\n",
      "next\n",
      "side\n",
      "time\n",
      "expects\n",
      "apparitions\n",
      "border\n",
      "nov\n",
      "european\n",
      "fuel\n",
      "california\n",
      "means\n",
      "upside\n",
      "plant\n",
      "down\n",
      "1968\n",
      "sea\n",
      "achievements\n",
      "can\n",
      "weeks\n",
      "humans\n",
      "mexico\n",
      "set\n",
      "shuttle\n",
      "spitzer\n",
      "lot\n",
      "gravity\n",
      "pay\n",
      "future\n",
      "launching\n",
      "happy\n",
      "sexual\n",
      "commander\n",
      "those\n",
      "apollo\n",
      "forming\n",
      "about\n",
      "sped\n",
      "ireland\n",
      "oli\n",
      "along\n",
      "london\n",
      "awakening\n",
      "shaped\n",
      "rocket\n",
      "suwannee\n",
      "jets\n",
      "month\n",
      "uss\n",
      "poses\n",
      "encounters\n",
      "aboard\n",
      "goes\n",
      "pilot\n",
      "account\n",
      "rolled\n",
      "way\n",
      "gear\n",
      "body\n",
      "launch\n",
      "paedophile\n",
      "mountainous\n",
      "historic\n",
      "ngc\n",
      "days\n",
      "filled\n",
      "round\n",
      "preparations\n",
      "texas\n",
      "giant\n",
      "served\n",
      "hague\n",
      "dusty\n",
      "cluster\n",
      "revealed\n",
      "1989\n",
      "men\n",
      "soyuz\n",
      "areas\n",
      "fog\n",
      "standard\n",
      "illegal\n",
      "around\n",
      "our\n",
      "washington\n",
      "his\n",
      "roughly\n",
      "discovery\n",
      "remind\n",
      "located\n",
      "agitator\n",
      "orion\n",
      "terrain\n",
      "but\n",
      "torus\n",
      "proposed\n",
      "black\n",
      "expedition\n",
      "data\n",
      "named\n",
      "dnieper\n",
      "module\n",
      "kennedy\n",
      "silhouette\n",
      "fresh\n",
      "region\n",
      "taken\n",
      "waters\n",
      "duarte\n",
      "false\n",
      "surroundings\n",
      "demonstrate\n",
      "research\n",
      "people\n",
      "dating\n",
      "enveloped\n",
      "features\n",
      "kepler\n",
      "flooding\n",
      "acaba\n",
      "celestial\n",
      "collecting\n",
      "shows\n",
      "per\n",
      "pacific\n",
      "recover\n",
      "splashes\n",
      "collins\n",
      "used\n",
      "operational\n",
      "thing\n",
      "cent\n",
      "parker\n",
      "blackwater\n",
      "nick\n",
      "brown\n",
      "russia\n",
      "needed\n",
      "viewed\n",
      "undocking\n",
      "right\n",
      "airglow\n",
      "capsule\n",
      "far\n",
      "irish\n",
      "criticises\n",
      "flight\n",
      "object\n",
      "northern\n",
      "krayniy\n",
      "run\n",
      "cygnus\n",
      "400\n",
      "validate\n",
      "reported\n",
      "democratic\n",
      "lands\n",
      "excessive\n",
      "footprint\n",
      "while\n",
      "glittering\n",
      "version\n",
      "feb\n",
      "professionals\n",
      "foot\n",
      "water\n",
      "large\n",
      "probe\n",
      "procedures\n",
      "most\n",
      "out\n",
      "triggered\n",
      "hardware\n",
      "since\n",
      "training\n",
      "crews\n",
      "families\n",
      "ricky\n",
      "cupola\n",
      "p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**p0V**\n",
      "river\n",
      "sts\n",
      "shuttle\n",
      "their\n",
      "galileo\n",
      "orbit\n",
      "national\n",
      "members\n",
      "ochoa\n",
      "which\n",
      "toward\n",
      "window\n",
      "celebrating\n",
      "heritage\n",
      "senator\n",
      "center\n",
      "landing\n",
      "captured\n",
      "glenn\n",
      "down\n",
      "1968\n",
      "apollo\n",
      "month\n",
      "soyuz\n",
      "around\n",
      "discovery\n",
      "expedition\n",
      "people\n",
      "right\n",
      "becoming\n",
      "hemisphere\n",
      "lies\n",
      "paranormal\n",
      "18th\n",
      "eight\n",
      "sept\n",
      "1995\n",
      "contributions\n",
      "outer\n",
      "arrived\n",
      "brilliant\n",
      "neutral\n",
      "ovchinin\n",
      "1898\n",
      "japanese\n",
      "oleg\n",
      "artemyev\n",
      "valleys\n",
      "six\n",
      "other\n",
      "central\n",
      "delivered\n",
      "buoyancy\n",
      "series\n",
      "roscosmos\n",
      "flowing\n",
      "have\n",
      "veggie\n",
      "deployed\n",
      "into\n",
      "known\n",
      "astronauts\n",
      "may\n",
      "imager\n",
      "magellanic\n",
      "investigators\n",
      "jupiter\n",
      "cosmonaut\n",
      "minutes\n",
      "arnold\n",
      "december\n",
      "facility\n",
      "exploration\n",
      "returned\n",
      "stars\n",
      "botany\n",
      "curling\n",
      "dark\n",
      "day\n",
      "magnified\n",
      "onboard\n",
      "alexey\n",
      "laden\n",
      "globular\n",
      "its\n",
      "service\n",
      "growth\n",
      "southern\n",
      "autumn\n",
      "women\n",
      "launched\n",
      "such\n",
      "inside\n",
      "ghost\n",
      "transfer\n",
      "gas\n",
      "embrace\n",
      "checklist\n",
      "feustel\n",
      "laboratory\n",
      "visible\n",
      "atlantis\n",
      "assist\n",
      "floating\n",
      "rain\n",
      "many\n",
      "installs\n",
      "drew\n",
      "ethereal\n",
      "material\n",
      "2017\n",
      "left\n",
      "john\n",
      "how\n",
      "color\n",
      "survival\n",
      "joe\n",
      "ball\n",
      "hours\n",
      "venus\n",
      "airbus\n",
      "might\n",
      "shipment\n",
      "bright\n",
      "nueces\n",
      "organic\n",
      "storms\n",
      "cassieopeia\n",
      "ago\n",
      "vehicle\n",
      "fire\n",
      "1998\n",
      "gulf\n",
      "solar\n",
      "perform\n",
      "world\n",
      "amounts\n",
      "not\n",
      "revolution\n",
      "1962\n",
      "cloud\n",
      "airport\n",
      "reading\n",
      "planet\n",
      "snow\n",
      "land\n",
      "photographed\n",
      "living\n",
      "landsat\n",
      "mile\n",
      "agency\n",
      "october\n",
      "ellen\n",
      "splashed\n",
      "spent\n",
      "9th\n",
      "glow\n",
      "team\n",
      "drifts\n",
      "side\n",
      "time\n",
      "apparitions\n",
      "nov\n",
      "european\n",
      "california\n",
      "means\n",
      "upside\n",
      "plant\n",
      "sea\n",
      "achievements\n",
      "mexico\n",
      "gravity\n",
      "those\n",
      "sped\n",
      "oli\n",
      "along\n",
      "awakening\n",
      "work\n",
      "suwannee\n",
      "jets\n",
      "poses\n",
      "aboard\n",
      "because\n",
      "gear\n",
      "mountainous\n",
      "historic\n",
      "ngc\n",
      "filled\n",
      "round\n",
      "preparations\n",
      "texas\n",
      "giant\n",
      "hague\n",
      "cluster\n",
      "1989\n",
      "men\n",
      "areas\n",
      "fog\n",
      "his\n",
      "remind\n",
      "terrain\n",
      "but\n",
      "black\n",
      "dnieper\n",
      "module\n",
      "kennedy\n",
      "taken\n",
      "waters\n",
      "false\n",
      "demonstrate\n",
      "enveloped\n",
      "flooding\n",
      "acaba\n",
      "operational\n",
      "parker\n",
      "blackwater\n",
      "nick\n",
      "brown\n",
      "russia\n",
      "viewed\n",
      "undocking\n",
      "airglow\n",
      "flight\n",
      "object\n",
      "northern\n",
      "krayniy\n",
      "400\n",
      "reported\n",
      "lands\n",
      "glittering\n",
      "feb\n",
      "professionals\n",
      "water\n",
      "large\n",
      "probe\n",
      "most\n",
      "since\n",
      "training\n",
      "crews\n",
      "families\n",
      "ricky\n",
      "cupola\n",
      "modern\n",
      "amber\n",
      "slip\n",
      "operations\n",
      "alexander\n",
      "office\n",
      "firefighters\n",
      "shadow\n",
      "condemnation\n",
      "hidden\n",
      "danger\n",
      "tragic\n",
      "five\n",
      "been\n",
      "condemns\n",
      "verify\n",
      "child\n",
      "conception\n",
      "west\n",
      "money\n",
      "composite\n",
      "pad\n",
      "decades\n",
      "tuesday\n",
      "yet\n",
      "nine\n",
      "give\n",
      "rules\n",
      "telescope\n",
      "plane\n",
      "europe\n",
      "cat\n",
      "three\n",
      "trust\n",
      "lure\n",
      "tests\n",
      "ocean\n",
      "street\n",
      "leader\n",
      "flat\n",
      "unacceptable\n",
      "two\n",
      "transport\n",
      "scorpius\n",
      "stagnating\n",
      "place\n",
      "accident\n",
      "centres\n",
      "investigation\n",
      "deep\n",
      "force\n",
      "growing\n",
      "missions\n",
      "speaking\n",
      "information\n",
      "sky\n",
      "prime\n",
      "letter\n",
      "unequal\n",
      "complain\n",
      "announces\n",
      "core\n",
      "insists\n",
      "rates\n",
      "family\n",
      "artist\n",
      "expectancy\n",
      "second\n",
      "sun\n",
      "year\n",
      "sunday\n",
      "trace\n",
      "further\n",
      "wake\n",
      "trip\n",
      "targeted\n",
      "night\n",
      "more\n",
      "even\n",
      "fifth\n",
      "video\n",
      "180\n",
      "ever\n",
      "impression\n",
      "countries\n",
      "science\n",
      "young\n",
      "arlene\n",
      "ahead\n",
      "momentum\n",
      "labour\n",
      "con\n",
      "wilderness\n",
      "parts\n",
      "blame\n",
      "memories\n",
      "birthday\n",
      "sunset\n",
      "gerst\n",
      "test\n",
      "calling\n",
      "train\n",
      "visits\n",
      "2015\n",
      "another\n",
      "planets\n",
      "productive\n",
      "couple\n",
      "issue\n",
      "constellation\n",
      "employees\n",
      "polled\n",
      "previously\n",
      "cartwright\n",
      "final\n",
      "old\n",
      "need\n",
      "latest\n",
      "stop\n",
      "before\n",
      "downing\n",
      "michael\n",
      "animated\n",
      "beyond\n",
      "article\n",
      "billions\n",
      "police\n",
      "star\n",
      "feline\n",
      "milky\n",
      "recovered\n",
      "forecast\n",
      "took\n",
      "brother\n",
      "paw\n",
      "greenpeace\n",
      "said\n",
      "one\n",
      "only\n",
      "party\n",
      "brexit\n",
      "deal\n",
      "expected\n",
      "working\n",
      "weather\n",
      "create\n",
      "nearly\n",
      "closure\n",
      "miles\n",
      "british\n",
      "public\n",
      "called\n",
      "stronger\n",
      "campaigners\n",
      "north\n",
      "wales\n",
      "fertility\n",
      "warning\n",
      "xavier\n",
      "better\n",
      "frozen\n",
      "ninety\n",
      "raise\n",
      "fierce\n",
      "them\n",
      "fighting\n",
      "judge\n",
      "home\n",
      "galaxy\n",
      "donut\n",
      "finds\n",
      "setting\n",
      "boris\n",
      "theresa\n",
      "campsfield\n",
      "older\n",
      "divided\n",
      "remembrance\n",
      "shared\n",
      "transits\n",
      "apps\n",
      "repurposed\n",
      "thought\n",
      "boss\n",
      "gemini\n",
      "command\n",
      "nebula\n",
      "murtha\n",
      "reflect\n",
      "history\n",
      "next\n",
      "metropolitan\n",
      "expects\n",
      "border\n",
      "don\n",
      "look\n",
      "johnson\n",
      "fuel\n",
      "twenty\n",
      "panic\n",
      "all\n",
      "entered\n",
      "trend\n",
      "can\n",
      "weeks\n",
      "humans\n",
      "raf\n",
      "set\n",
      "mean\n",
      "longest\n",
      "spitzer\n",
      "lot\n",
      "pay\n",
      "future\n",
      "launching\n",
      "happy\n",
      "sexual\n",
      "commander\n",
      "bells\n",
      "forming\n",
      "about\n",
      "foster\n",
      "ireland\n",
      "london\n",
      "shaped\n",
      "rocket\n",
      "referendum\n",
      "uss\n",
      "new\n",
      "encounters\n",
      "goes\n",
      "pilot\n",
      "now\n",
      "war\n",
      "account\n",
      "rolled\n",
      "interpol\n",
      "way\n",
      "body\n",
      "increases\n",
      "launch\n",
      "paedophile\n",
      "days\n",
      "keep\n",
      "falling\n",
      "served\n",
      "dusty\n",
      "revealed\n",
      "navigator\n",
      "south\n",
      "met\n",
      "following\n",
      "standard\n",
      "illegal\n",
      "our\n",
      "washington\n",
      "roughly\n",
      "via\n",
      "commented\n",
      "located\n",
      "agitator\n",
      "orion\n",
      "torus\n",
      "than\n",
      "proposed\n",
      "data\n",
      "named\n",
      "fall\n",
      "silhouette\n",
      "will\n",
      "fresh\n",
      "region\n",
      "attacker\n",
      "duarte\n",
      "raises\n",
      "surroundings\n",
      "research\n",
      "dating\n",
      "features\n",
      "kepler\n",
      "life\n",
      "watch\n",
      "celestial\n",
      "collecting\n",
      "sweden\n",
      "shows\n",
      "say\n",
      "less\n",
      "per\n",
      "pacific\n",
      "recover\n",
      "islington\n",
      "splashes\n",
      "collins\n",
      "used\n",
      "thing\n",
      "cent\n",
      "bed\n",
      "house\n",
      "decorated\n",
      "migration\n",
      "needed\n",
      "capsule\n",
      "far\n",
      "seconds\n",
      "irish\n",
      "criticises\n",
      "today\n",
      "run\n",
      "cygnus\n",
      "validate\n",
      "conditions\n",
      "democratic\n",
      "excessive\n",
      "female\n",
      "footprint\n",
      "while\n",
      "version\n",
      "chancellor\n",
      "short\n",
      "foot\n",
      "some\n",
      "britain\n",
      "procedures\n",
      "out\n",
      "alarm\n",
      "triggered\n",
      "hardware\n",
      "independent\n",
      "concludes\n"
     ]
    }
   ],
   "source": [
    "getTopWords(nasa, news)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
